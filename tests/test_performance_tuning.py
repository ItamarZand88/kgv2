#!/usr/bin/env python3

import asyncio
import sys
import os
import time
import logging

# Add the current directory to Python path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from core import CodeAnalyzer
from storage import StorageManager, GraphManager

# Configure logging to see what's happening
logging.basicConfig(level=logging.INFO)

async def test_performance_steps():
    """Test each step of the analysis process to find bottlenecks"""
    
    print("ğŸš€ Performance Tuning Test")
    print("=" * 60)
    
    storage = StorageManager('')
    graph_mgr = GraphManager(storage)
    analyzer = CodeAnalyzer(storage, graph_mgr)
    
    test_path = "./test_repo"
    repo_id = f"perf_test_{int(time.time())}"
    
    print(f"ğŸ“ Repository: {test_path}")
    print(f"ğŸ†” Repo ID: {repo_id}")
    
    # Step 1: File discovery
    print("\nğŸ” Step 1: File Discovery")
    start = time.time()
    files = await analyzer._find_source_files(test_path)
    step1_time = time.time() - start
    print(f"   âœ… Found {len(files)} files in {step1_time:.3f}s")
    
    # Step 2: File parsing (individual files)
    print("\nğŸ” Step 2: File Parsing")
    start = time.time()
    all_nodes = []
    all_edges = []
    
    for file_path in files:
        relative_path = analyzer._get_relative_path(test_path, file_path)
        nodes, edges = await analyzer._analyze_file(file_path, relative_path, repo_id)
        all_nodes.extend(nodes)
        all_edges.extend(edges)
    
    step2_time = time.time() - start
    print(f"   âœ… Parsed {len(files)} files in {step2_time:.3f}s")
    print(f"   ğŸ“Š Found {len(all_nodes)} nodes, {len(all_edges)} edges")
    print(f"   ğŸ“Š Global entities: {len(analyzer.global_entities)}")
    
    # Step 3: NetworkX graph building
    print("\nğŸ” Step 3: NetworkX Graph Building")
    start = time.time()
    analyzer._build_networkx_graph(all_nodes, all_edges)
    step3_time = time.time() - start
    print(f"   âœ… Built NetworkX graph in {step3_time:.3f}s")
    print(f"   ğŸ“Š Nodes: {analyzer.nx_graph.number_of_nodes()}")
    print(f"   ğŸ“Š Edges: {analyzer.nx_graph.number_of_edges()}")
    
    # Step 4: PageRank calculation
    print("\nğŸ” Step 4: PageRank Calculation")
    start = time.time()
    ranked_entities = analyzer._apply_pagerank_ranking()
    step4_time = time.time() - start
    print(f"   âœ… PageRank completed in {step4_time:.3f}s")
    print(f"   ğŸ“Š Ranked {len(ranked_entities)} entities")
    
    # Step 5: Cross-file relationship analysis
    print("\nğŸ” Step 5: Cross-file Relationships")
    start = time.time()
    cross_file_edges = analyzer._create_cross_file_edges_with_ranking(ranked_entities)
    step5_time = time.time() - start
    print(f"   âœ… Cross-file analysis in {step5_time:.3f}s")
    print(f"   ğŸ“Š Created {len(cross_file_edges)} cross-file edges")
    
    # Step 6: Database persistence (this might be the bottleneck!)
    print("\nğŸ” Step 6: Database Persistence")
    start = time.time()
    
    # Create a mock results object
    results = {
        'nodes_created': 0,
        'edges_created': 0,
        'errors': []
    }
    
    await analyzer._persist_analysis_results(repo_id, all_nodes, all_edges + cross_file_edges, results)
    step6_time = time.time() - start
    print(f"   âœ… Persisted to database in {step6_time:.3f}s")
    print(f"   ğŸ“Š Stored {results['nodes_created']} nodes, {results['edges_created']} edges")
    
    # Summary
    total_time = step1_time + step2_time + step3_time + step4_time + step5_time + step6_time
    
    print("\n" + "=" * 60)
    print("âš¡ PERFORMANCE SUMMARY")
    print("=" * 60)
    print(f"1. File Discovery:       {step1_time:.3f}s ({step1_time/total_time*100:.1f}%)")
    print(f"2. File Parsing:         {step2_time:.3f}s ({step2_time/total_time*100:.1f}%)")
    print(f"3. NetworkX Building:    {step3_time:.3f}s ({step3_time/total_time*100:.1f}%)")
    print(f"4. PageRank:             {step4_time:.3f}s ({step4_time/total_time*100:.1f}%)")
    print(f"5. Cross-file Analysis:  {step5_time:.3f}s ({step5_time/total_time*100:.1f}%)")
    print(f"6. Database Persistence: {step6_time:.3f}s ({step6_time/total_time*100:.1f}%)")
    print(f"   TOTAL:                {total_time:.3f}s")
    
    # Identify bottlenecks
    times = [
        ("File Discovery", step1_time),
        ("File Parsing", step2_time), 
        ("NetworkX Building", step3_time),
        ("PageRank", step4_time),
        ("Cross-file Analysis", step5_time),
        ("Database Persistence", step6_time)
    ]
    
    # Find the slowest step
    slowest_step, slowest_time = max(times, key=lambda x: x[1])
    
    print(f"\nğŸŒ BOTTLENECK: {slowest_step} ({slowest_time:.3f}s)")
    
    if slowest_step == "Database Persistence":
        print("   ğŸ’¡ Suggestion: Optimize batch sizes or use bulk operations")
    elif slowest_step == "PageRank":
        print("   ğŸ’¡ Suggestion: Skip PageRank for large repos or use approximation")
    elif slowest_step == "Cross-file Analysis":
        print("   ğŸ’¡ Suggestion: Limit cross-file analysis scope")
    elif slowest_step == "File Parsing":
        print("   ğŸ’¡ Suggestion: Increase parallelism or filter files")

if __name__ == "__main__":
    asyncio.run(test_performance_steps()) 